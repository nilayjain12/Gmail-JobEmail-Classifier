{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9866978,"sourceType":"datasetVersion","datasetId":6056459},{"sourceId":9877048,"sourceType":"datasetVersion","datasetId":6063924}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas numpy sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:52:55.562256Z","iopub.execute_input":"2024-11-13T22:52:55.562674Z","iopub.status.idle":"2024-11-13T22:53:08.876955Z","shell.execute_reply.started":"2024-11-13T22:52:55.562641Z","shell.execute_reply":"2024-11-13T22:53:08.875957Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# from backend import config\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:08:34.566482Z","iopub.execute_input":"2024-11-14T00:08:34.567282Z","iopub.status.idle":"2024-11-14T00:08:34.572745Z","shell.execute_reply.started":"2024-11-14T00:08:34.567240Z","shell.execute_reply":"2024-11-14T00:08:34.571658Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"# Load the CSV email dataset\ndata_frame = pd.read_csv('/kaggle/input/jobemailstatus-dataset/emails_dataset.csv')\ndata_frame = data_frame.sample(frac=1)\ndata_frame.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:08:35.782703Z","iopub.execute_input":"2024-11-14T00:08:35.783414Z","iopub.status.idle":"2024-11-14T00:08:35.823886Z","shell.execute_reply.started":"2024-11-14T00:08:35.783373Z","shell.execute_reply":"2024-11-14T00:08:35.823001Z"}},"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"                                                subject  \\\n3141          Interview Invitation for Your Application   \n1839  Congratulations! You've Been Scheduled for an ...   \n2285     Application Received - Thank you for applying!   \n2601               Interview Scheduled - Exciting News!   \n2997            Your Job Application has been submitted   \n\n                                                message  label  \n3141  Dear Candidate, We are excited to invite you t...      2  \n1839  Hi, Congratulations! You have progressed to th...      2  \n2285  Dear Applicant, Thank you for applying to our ...      0  \n2601  Hello, We are pleased to schedule an interview...      2  \n2997  Dear Applicant, Thank you for applying to our ...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>message</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3141</th>\n      <td>Interview Invitation for Your Application</td>\n      <td>Dear Candidate, We are excited to invite you t...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1839</th>\n      <td>Congratulations! You've Been Scheduled for an ...</td>\n      <td>Hi, Congratulations! You have progressed to th...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2285</th>\n      <td>Application Received - Thank you for applying!</td>\n      <td>Dear Applicant, Thank you for applying to our ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2601</th>\n      <td>Interview Scheduled - Exciting News!</td>\n      <td>Hello, We are pleased to schedule an interview...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Your Job Application has been submitted</td>\n      <td>Dear Applicant, Thank you for applying to our ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":125},{"cell_type":"code","source":"data_frame.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:08:38.482703Z","iopub.execute_input":"2024-11-14T00:08:38.483561Z","iopub.status.idle":"2024-11-14T00:08:38.493692Z","shell.execute_reply.started":"2024-11-14T00:08:38.483518Z","shell.execute_reply":"2024-11-14T00:08:38.492987Z"}},"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"subject    0\nmessage    0\nlabel      0\ndtype: int64"},"metadata":{}}],"execution_count":126},{"cell_type":"code","source":"# Count of each label\ndata_frame['message'].groupby(data_frame['label']).count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:08:39.826468Z","iopub.execute_input":"2024-11-14T00:08:39.826829Z","iopub.status.idle":"2024-11-14T00:08:39.837478Z","shell.execute_reply.started":"2024-11-14T00:08:39.826795Z","shell.execute_reply":"2024-11-14T00:08:39.836393Z"}},"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"label\n0    819\n1    776\n2    734\n3    822\nName: message, dtype: int64"},"metadata":{}}],"execution_count":127},{"cell_type":"code","source":"!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:53:27.444937Z","iopub.execute_input":"2024-11-13T22:53:27.445235Z","iopub.status.idle":"2024-11-13T22:53:39.129122Z","shell.execute_reply.started":"2024-11-13T22:53:27.445206Z","shell.execute_reply":"2024-11-13T22:53:39.128008Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Download the essential NLTK files for stopwords, punctuations, and lemmatization\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport string\n\n# Download necessary resources\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('wordnet')\nprint('Successfully Executed!!!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:08:42.755663Z","iopub.execute_input":"2024-11-14T00:08:42.756044Z","iopub.status.idle":"2024-11-14T00:08:43.017679Z","shell.execute_reply.started":"2024-11-14T00:08:42.756008Z","shell.execute_reply":"2024-11-14T00:08:43.016755Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nSuccessfully Executed!!!\n","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:08:44.794369Z","iopub.execute_input":"2024-11-14T00:08:44.794728Z","iopub.status.idle":"2024-11-14T00:11:30.634111Z","shell.execute_reply.started":"2024-11-14T00:08:44.794695Z","shell.execute_reply":"2024-11-14T00:11:30.633236Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\nreplace /usr/share/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"# Initialize stop words and lemmatizer\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Function for removing stopwords, lemmaztize and tokenize the data\ndef clean_text(text):\n    # Tokenize text\n    tokens = word_tokenize(text.lower())  # Convert to lowercase\n    # Remove stopwords and punctuation, then lemmatize\n    cleaned_tokens = [\n        lemmatizer.lemmatize(word) for word in tokens\n        if word not in stop_words and word not in string.punctuation\n    ]\n    return ' '.join(cleaned_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:30.636298Z","iopub.execute_input":"2024-11-14T00:11:30.636681Z","iopub.status.idle":"2024-11-14T00:11:30.645361Z","shell.execute_reply.started":"2024-11-14T00:11:30.636642Z","shell.execute_reply":"2024-11-14T00:11:30.644484Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"# Creating a copy of the original data frame for a separate tokanized data frame\ndata_frame_tokanized = data_frame.copy()\ndata_frame_tokanized.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:30.646489Z","iopub.execute_input":"2024-11-14T00:11:30.646774Z","iopub.status.idle":"2024-11-14T00:11:30.663349Z","shell.execute_reply.started":"2024-11-14T00:11:30.646744Z","shell.execute_reply":"2024-11-14T00:11:30.662182Z"}},"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"                                                subject  \\\n3141          Interview Invitation for Your Application   \n1839  Congratulations! You've Been Scheduled for an ...   \n2285     Application Received - Thank you for applying!   \n\n                                                message  label  \n3141  Dear Candidate, We are excited to invite you t...      2  \n1839  Hi, Congratulations! You have progressed to th...      2  \n2285  Dear Applicant, Thank you for applying to our ...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>message</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3141</th>\n      <td>Interview Invitation for Your Application</td>\n      <td>Dear Candidate, We are excited to invite you t...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1839</th>\n      <td>Congratulations! You've Been Scheduled for an ...</td>\n      <td>Hi, Congratulations! You have progressed to th...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2285</th>\n      <td>Application Received - Thank you for applying!</td>\n      <td>Dear Applicant, Thank you for applying to our ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":131},{"cell_type":"code","source":"# Apply the function to the 'subject' and 'messages' columns and store in 'tokenized_subject' and 'tokenized_messages'\ndata_frame_tokanized['tokenized_subject'] = data_frame_tokanized['subject'].apply(clean_text)\ndata_frame_tokanized['tokenized_messages'] = data_frame_tokanized['message'].apply(clean_text)\ndata_frame_tokanized = data_frame_tokanized.drop(['subject', 'message'], axis=1)\n\ndata_frame_tokanized.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:30.666059Z","iopub.execute_input":"2024-11-14T00:11:30.666346Z","iopub.status.idle":"2024-11-14T00:11:32.718209Z","shell.execute_reply.started":"2024-11-14T00:11:30.666314Z","shell.execute_reply":"2024-11-14T00:11:32.717102Z"}},"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"      label                       tokenized_subject  \\\n3141      2        interview invitation application   \n1839      2  congratulation 've scheduled interview   \n2285      0     application received thank applying   \n2601      2       interview scheduled exciting news   \n2997      0               job application submitted   \n\n                                     tokenized_messages  \n3141  dear candidate excited invite interview please...  \n1839  hi congratulation progressed interview stage p...  \n2285  dear applicant thank applying company received...  \n2601  hello pleased schedule interview kindly see at...  \n2997  dear applicant thank applying company received...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tokenized_subject</th>\n      <th>tokenized_messages</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3141</th>\n      <td>2</td>\n      <td>interview invitation application</td>\n      <td>dear candidate excited invite interview please...</td>\n    </tr>\n    <tr>\n      <th>1839</th>\n      <td>2</td>\n      <td>congratulation 've scheduled interview</td>\n      <td>hi congratulation progressed interview stage p...</td>\n    </tr>\n    <tr>\n      <th>2285</th>\n      <td>0</td>\n      <td>application received thank applying</td>\n      <td>dear applicant thank applying company received...</td>\n    </tr>\n    <tr>\n      <th>2601</th>\n      <td>2</td>\n      <td>interview scheduled exciting news</td>\n      <td>hello pleased schedule interview kindly see at...</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>0</td>\n      <td>job application submitted</td>\n      <td>dear applicant thank applying company received...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":132},{"cell_type":"code","source":"# Concatenate the tokenized_subject and tokenized_messages\ndata_frame_tokanized['email_content'] = data_frame_tokanized['tokenized_subject'] + data_frame_tokanized['tokenized_messages']\n\ndata_frame_tokanized.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:32.719875Z","iopub.execute_input":"2024-11-14T00:11:32.720311Z","iopub.status.idle":"2024-11-14T00:11:32.733334Z","shell.execute_reply.started":"2024-11-14T00:11:32.720267Z","shell.execute_reply":"2024-11-14T00:11:32.732220Z"}},"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"      label                       tokenized_subject  \\\n3141      2        interview invitation application   \n1839      2  congratulation 've scheduled interview   \n2285      0     application received thank applying   \n\n                                     tokenized_messages  \\\n3141  dear candidate excited invite interview please...   \n1839  hi congratulation progressed interview stage p...   \n2285  dear applicant thank applying company received...   \n\n                                          email_content  \n3141  interview invitation applicationdear candidate...  \n1839  congratulation 've scheduled interviewhi congr...  \n2285  application received thank applyingdear applic...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>tokenized_subject</th>\n      <th>tokenized_messages</th>\n      <th>email_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3141</th>\n      <td>2</td>\n      <td>interview invitation application</td>\n      <td>dear candidate excited invite interview please...</td>\n      <td>interview invitation applicationdear candidate...</td>\n    </tr>\n    <tr>\n      <th>1839</th>\n      <td>2</td>\n      <td>congratulation 've scheduled interview</td>\n      <td>hi congratulation progressed interview stage p...</td>\n      <td>congratulation 've scheduled interviewhi congr...</td>\n    </tr>\n    <tr>\n      <th>2285</th>\n      <td>0</td>\n      <td>application received thank applying</td>\n      <td>dear applicant thank applying company received...</td>\n      <td>application received thank applyingdear applic...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":133},{"cell_type":"code","source":"data_frame_preprocessed = data_frame_tokanized.copy()\ndata_frame_preprocessed = data_frame_preprocessed.drop(['tokenized_subject', 'tokenized_messages'], axis=1)\ndata_frame_preprocessed = data_frame_preprocessed.sample(frac=1)\n# Final Data Frame\ndata_frame_preprocessed.head(5), data_frame_preprocessed.shape, data_frame_preprocessed.info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:32.734649Z","iopub.execute_input":"2024-11-14T00:11:32.734987Z","iopub.status.idle":"2024-11-14T00:11:32.748952Z","shell.execute_reply.started":"2024-11-14T00:11:32.734949Z","shell.execute_reply":"2024-11-14T00:11:32.748161Z"}},"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"(      label                                      email_content\n 1317      3  application status decisiondear applicant care...\n 104       0  appreciate interest joining brightmindhi drew ...\n 385       1  next step ux designer assessmentdear taylor pa...\n 2887      0  've received applicationhello received applica...\n 1198      1  invitation complete assessmentcongratulation p...,\n (3151, 2),\n <bound method DataFrame.info of       label                                      email_content\n 1317      3  application status decisiondear applicant care...\n 104       0  appreciate interest joining brightmindhi drew ...\n 385       1  next step ux designer assessmentdear taylor pa...\n 2887      0  've received applicationhello received applica...\n 1198      1  invitation complete assessmentcongratulation p...\n ...     ...                                                ...\n 335       1  complete data scientist assessmenthi alex ’ ex...\n 1621      0  job application submitteddear applicant thank ...\n 1763      0  job application submittedhi application receiv...\n 1379      3  update application statushello thank interest ...\n 166       0  application review innovatexhello taylor wante...\n \n [3151 rows x 2 columns]>)"},"metadata":{}}],"execution_count":134},{"cell_type":"code","source":"!pip install torch transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:53:44.658305Z","iopub.execute_input":"2024-11-13T22:53:44.658626Z","iopub.status.idle":"2024-11-13T22:53:56.419748Z","shell.execute_reply.started":"2024-11-13T22:53:44.658593Z","shell.execute_reply":"2024-11-13T22:53:56.418540Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Import Libraries\nimport pandas as pd\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom transformers import LongformerTokenizer, LongformerForSequenceClassification\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:32.749987Z","iopub.execute_input":"2024-11-14T00:11:32.750294Z","iopub.status.idle":"2024-11-14T00:11:32.771066Z","shell.execute_reply.started":"2024-11-14T00:11:32.750260Z","shell.execute_reply":"2024-11-14T00:11:32.770262Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"# Prepare Data\n# Assuming data_frame_preprocessed is already loaded\ndata_frame_final = data_frame_preprocessed[['label', 'email_content']]\n\n# Split the data into training and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    data_frame_final['email_content'].tolist(), \n    data_frame_final['label'].tolist(), \n    test_size=0.2, \n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:42.111244Z","iopub.execute_input":"2024-11-14T00:11:42.112111Z","iopub.status.idle":"2024-11-14T00:11:42.122094Z","shell.execute_reply.started":"2024-11-14T00:11:42.112071Z","shell.execute_reply":"2024-11-14T00:11:42.121047Z"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"# Tokenize Data using DistilBert\ntokenizer_DB = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the text\ntrain_encodings = tokenizer_DB(train_texts, truncation=True, padding=True, max_length=1024)\ntest_encodings = tokenizer_DB(test_texts, truncation=True, padding=True, max_length=1024)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize Data using Bert\ntokenizer_BERT = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the text\ntrain_encodings = tokenizer_BERT(train_texts, truncation=True, padding=True, max_length=2048)\ntest_encodings = tokenizer_BERT(test_texts, truncation=True, padding=True, max_length=2048)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T23:53:46.215290Z","iopub.execute_input":"2024-11-13T23:53:46.215717Z","iopub.status.idle":"2024-11-13T23:53:49.190306Z","shell.execute_reply.started":"2024-11-13T23:53:46.215676Z","shell.execute_reply":"2024-11-13T23:53:49.189267Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"# Tokenize using Longformer tokenizer\ntokenizer_LF = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n\n# Tokenize the text with max_length of 2048\ntrain_encodings = tokenizer_LF(train_texts, truncation=True, padding=True, max_length=2048)\ntest_encodings = tokenizer_LF(test_texts, truncation=True, padding=True, max_length=2048)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:48.075279Z","iopub.execute_input":"2024-11-14T00:11:48.075668Z","iopub.status.idle":"2024-11-14T00:11:49.057646Z","shell.execute_reply.started":"2024-11-14T00:11:48.075629Z","shell.execute_reply":"2024-11-14T00:11:49.056787Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"# Create PyTorch Dataset\nclass EmailDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = EmailDataset(train_encodings, train_labels)\ntest_dataset = EmailDataset(test_encodings, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:51.882981Z","iopub.execute_input":"2024-11-14T00:11:51.883362Z","iopub.status.idle":"2024-11-14T00:11:51.891312Z","shell.execute_reply.started":"2024-11-14T00:11:51.883325Z","shell.execute_reply":"2024-11-14T00:11:51.890319Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"# Initialize DistilBert Model\nmodel_DB = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Bert Model\nmodel_BERT = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T23:53:50.314949Z","iopub.execute_input":"2024-11-13T23:53:50.315594Z","iopub.status.idle":"2024-11-13T23:53:50.520046Z","shell.execute_reply.started":"2024-11-13T23:53:50.315553Z","shell.execute_reply":"2024-11-13T23:53:50.519134Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"# Initialize LongFormer Model\nmodel_LF = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:11:54.827491Z","iopub.execute_input":"2024-11-14T00:11:54.827926Z","iopub.status.idle":"2024-11-14T00:12:03.383026Z","shell.execute_reply.started":"2024-11-14T00:11:54.827885Z","shell.execute_reply":"2024-11-14T00:12:03.382080Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ccfb76267634ec4ba862f962876e1ff"}},"metadata":{}},{"name":"stderr","text":"Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"# Define Evaluation Metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:12:14.784300Z","iopub.execute_input":"2024-11-14T00:12:14.785033Z","iopub.status.idle":"2024-11-14T00:12:14.791390Z","shell.execute_reply.started":"2024-11-14T00:12:14.784991Z","shell.execute_reply":"2024-11-14T00:12:14.790537Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"# Training Arguments For Bert\ntraining_args_B = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_dir='./logs'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T23:53:55.212108Z","iopub.execute_input":"2024-11-13T23:53:55.212884Z","iopub.status.idle":"2024-11-13T23:53:55.251306Z","shell.execute_reply.started":"2024-11-13T23:53:55.212821Z","shell.execute_reply":"2024-11-13T23:53:55.250368Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"# Training Arguments For DistilBert\ntraining_args_DB = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_dir='./logs'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T23:29:06.994667Z","iopub.execute_input":"2024-11-13T23:29:06.995054Z","iopub.status.idle":"2024-11-13T23:29:07.032271Z","shell.execute_reply.started":"2024-11-13T23:29:06.995017Z","shell.execute_reply":"2024-11-13T23:29:07.031267Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Training Arguments For LongFormer\ntraining_args_LF = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_dir='./logs'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:12:47.627687Z","iopub.execute_input":"2024-11-14T00:12:47.628096Z","iopub.status.idle":"2024-11-14T00:12:47.664615Z","shell.execute_reply.started":"2024-11-14T00:12:47.628058Z","shell.execute_reply":"2024-11-14T00:12:47.663673Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":143},{"cell_type":"code","source":"# Train the Bert Model\ntrainer_B = Trainer(\n    model=model_BERT,\n    args=training_args_B,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer_B.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T23:53:58.407020Z","iopub.execute_input":"2024-11-13T23:53:58.407410Z","iopub.status.idle":"2024-11-14T00:00:27.174440Z","shell.execute_reply.started":"2024-11-13T23:53:58.407374Z","shell.execute_reply":"2024-11-14T00:00:27.173488Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1580/1580 06:27, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.000824</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.000363</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.000223</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.038000</td>\n      <td>0.000159</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.038000</td>\n      <td>0.000124</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.038000</td>\n      <td>0.000103</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000200</td>\n      <td>0.000090</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000200</td>\n      <td>0.000082</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000200</td>\n      <td>0.000077</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000100</td>\n      <td>0.000076</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1580, training_loss=0.012131533281924792, metrics={'train_runtime': 388.0153, 'train_samples_per_second': 64.946, 'train_steps_per_second': 4.072, 'total_flos': 841764937104000.0, 'train_loss': 0.012131533281924792, 'epoch': 10.0})"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"# Train the LongFormer Model\ntrainer_LF = Trainer(\n    model=model_LF,\n    args=training_args_LF,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer_LF.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:13:26.326539Z","iopub.execute_input":"2024-11-14T00:13:26.327481Z","iopub.status.idle":"2024-11-14T01:08:38.809742Z","shell.execute_reply.started":"2024-11-14T00:13:26.327428Z","shell.execute_reply":"2024-11-14T01:08:38.808876Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nInitializing global attention on CLS token...\nInitializing global attention on CLS token...\nInput ids are automatically padded to be a multiple of `config.attention_window`: 512\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1580/1580 55:09, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.000340</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.000160</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.000102</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.033800</td>\n      <td>0.000074</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.033800</td>\n      <td>0.000059</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.033800</td>\n      <td>0.000049</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000100</td>\n      <td>0.000043</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>0.000040</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000100</td>\n      <td>0.000037</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000100</td>\n      <td>0.000037</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1580, training_loss=0.010762295968052518, metrics={'train_runtime': 3311.6655, 'train_samples_per_second': 7.609, 'train_steps_per_second': 0.477, 'total_flos': 1083050991792000.0, 'train_loss': 0.010762295968052518, 'epoch': 10.0})"},"metadata":{}}],"execution_count":144},{"cell_type":"code","source":"# Train the DistilBert Model\ntrainer_DB = Trainer(\n    model=model_DB,\n    args=training_args_DB,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics\n)\n\ntrainer_DB.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:54:01.972601Z","iopub.execute_input":"2024-11-13T22:54:01.972964Z","iopub.status.idle":"2024-11-13T22:58:39.735742Z","shell.execute_reply.started":"2024-11-13T22:54:01.972928Z","shell.execute_reply":"2024-11-13T22:58:39.734924Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112851555556821, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93883fd58e164a5d8e999eaf566349c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241113_225507-tltnd9aj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nilay-jain12-california-state-university-fullerton/huggingface/runs/tltnd9aj' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/nilay-jain12-california-state-university-fullerton/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nilay-jain12-california-state-university-fullerton/huggingface' target=\"_blank\">https://wandb.ai/nilay-jain12-california-state-university-fullerton/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nilay-jain12-california-state-university-fullerton/huggingface/runs/tltnd9aj' target=\"_blank\">https://wandb.ai/nilay-jain12-california-state-university-fullerton/huggingface/runs/tltnd9aj</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1580/1580 03:27, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.001547</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.000489</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.000247</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.043700</td>\n      <td>0.000155</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.043700</td>\n      <td>0.000110</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.043700</td>\n      <td>0.000085</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000200</td>\n      <td>0.000070</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000200</td>\n      <td>0.000062</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000200</td>\n      <td>0.000057</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000100</td>\n      <td>0.000055</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1580, training_loss=0.01392982932002154, metrics={'train_runtime': 275.1346, 'train_samples_per_second': 91.592, 'train_steps_per_second': 5.743, 'total_flos': 423807301008000.0, 'train_loss': 0.01392982932002154, 'epoch': 10.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Evaluate the Bert Model\n# Evaluate the model on the test dataset\neval_results = trainer_B.evaluate()\nprint(\"Evaluation Results:\", eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T00:01:18.423195Z","iopub.execute_input":"2024-11-14T00:01:18.423586Z","iopub.status.idle":"2024-11-14T00:01:21.457134Z","shell.execute_reply.started":"2024-11-14T00:01:18.423550Z","shell.execute_reply":"2024-11-14T00:01:21.456130Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 7.568518776679412e-05, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 3.0233, 'eval_samples_per_second': 208.711, 'eval_steps_per_second': 13.23, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# Evaluate the DistilBert Model\n# Evaluate the model on the test dataset\neval_results = trainer_DB.evaluate()\nprint(\"Evaluation Results:\", eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:58:39.794654Z","iopub.execute_input":"2024-11-13T22:58:39.795052Z","iopub.status.idle":"2024-11-13T22:58:41.477382Z","shell.execute_reply.started":"2024-11-13T22:58:39.795004Z","shell.execute_reply":"2024-11-13T22:58:41.476496Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 5.521651837625541e-05, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 1.6714, 'eval_samples_per_second': 377.52, 'eval_steps_per_second': 23.932, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Evaluate the LongFormer Model\n# Evaluate the model on the test dataset\neval_results = trainer_LF.evaluate()\nprint(\"Evaluation Results:\", eval_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T01:09:07.679238Z","iopub.execute_input":"2024-11-14T01:09:07.679913Z","iopub.status.idle":"2024-11-14T01:09:41.184756Z","shell.execute_reply.started":"2024-11-14T01:09:07.679866Z","shell.execute_reply":"2024-11-14T01:09:41.183822Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:32]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 3.663305687950924e-05, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 33.4953, 'eval_samples_per_second': 18.838, 'eval_steps_per_second': 1.194, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"# from transformers import pipeline\n\n# # Load the summarization pipeline\n# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Test with a sample email content\nsample_text = \"\"\"\nShowcase Your Tech Passion with Cisco's Assessment Challenge!\nCisco Jobs\nHi Nilay,\n\nIt's an absolute pleasure to meet you! Here at Cisco, we're passionate about discovering exceptional talent in innovative ways.\n\nWe invite you to take part in our skill assessment challenge for the Software Engineer I (Full Time) United States role. This is your opportunity to shine and show us what you can do!\n\nStart your challenge here: Assessment Link\nYour Candidate ID to access the assessment is: 6404400\n\nThe assessment should take around 1 Hour 30 Minutes  to complete. Remember, it's important to work independently, so find a quiet space, minimize distractions, and ensure a smooth experience with Google Chrome or Mozilla Firefox.\n\nA few things to note:\n\n\t•\tIf you've recently completed this challenge for another role, you won't need to                    retake it, all you need to do is click the link for the data to integrate.\n\t•\tEven if you don't finish, your results will still be considered.\n\t•\tWhile the assessment informs our hiring process, it's not a strict pass/fail test.\n\t•\tFor the best results, complete the assessment within the next two weeks.\n\nShould any technical issues arise during the test, SHL Technical Support is available via live chat on their website. If issues persist, or you have further questions, reach out to Cisco Recruiting at assessment_help@cisco.com.\n\nAfter completing the assessment, you can check your application status on the Cisco Job Portal in a week.\n\nWe can't wait to see what you bring to the table!\n\nCheers,\nCisco Join & Connect\n#WeAreCisco\n\nEmail ID 32792\nThis address is unattended and cannot help with questions or requests. To receive these auto-notifications, please add donotreply@cisco.avature.net to your address book.\nFollow We Are Cisco Instagram Twitter Facebook Linkedin  YouTube\n©2024 Cisco and/or its affiliates\tJobs Help  Cisco.com  Privacy Statement  Trademarks\n\n\"\"\"\n\n# # Summarize the text\n# summary = summarizer(sample_text, max_length=100, min_length=30, do_sample=False)\n\n# email_summary = summary[0]['summary_text']\n# print(email_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T01:14:11.384351Z","iopub.execute_input":"2024-11-14T01:14:11.385036Z","iopub.status.idle":"2024-11-14T01:14:11.392335Z","shell.execute_reply.started":"2024-11-14T01:14:11.384996Z","shell.execute_reply":"2024-11-14T01:14:11.391445Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"# Predict on New Text\n\n# Determine the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the device\nmodel_LF.to(device)\n\n# Update the predict_label function to use the same device\ndef predict_label(text):\n    inputs = tokenizer_LF(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    # Move the inputs to the same device as the model\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    outputs = model_LF(**inputs)\n    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n    return predicted_label\n\nprint(\"Predicted Label:\", predict_label(sample_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T01:14:13.787017Z","iopub.execute_input":"2024-11-14T01:14:13.788075Z","iopub.status.idle":"2024-11-14T01:14:13.915805Z","shell.execute_reply.started":"2024-11-14T01:14:13.788034Z","shell.execute_reply":"2024-11-14T01:14:13.914916Z"}},"outputs":[{"name":"stdout","text":"Predicted Label: 1\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"# Saving the Fine Tuned Model\nmodel_LF.save_pretrained(\"./Mail_LongFormer-Classifier-4096\")\n# Save the tokenizer\ntokenizer_LF.save_pretrained(\"./Mail_LongFormer-Classifier-4096\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T01:20:23.659771Z","iopub.execute_input":"2024-11-14T01:20:23.660513Z","iopub.status.idle":"2024-11-14T01:20:25.219267Z","shell.execute_reply.started":"2024-11-14T01:20:23.660471Z","shell.execute_reply":"2024-11-14T01:20:25.218375Z"}},"outputs":[{"execution_count":161,"output_type":"execute_result","data":{"text/plain":"('./Mail_LongFormer-Classifier-4096/tokenizer_config.json',\n './Mail_LongFormer-Classifier-4096/special_tokens_map.json',\n './Mail_LongFormer-Classifier-4096/vocab.json',\n './Mail_LongFormer-Classifier-4096/merges.txt',\n './Mail_LongFormer-Classifier-4096/added_tokens.json')"},"metadata":{}}],"execution_count":161},{"cell_type":"code","source":"import shutil\n\n# Zip the model directory\nshutil.make_archive('./Mail_LongFormer-Classifier-4096', 'zip', './Mail_LongFormer-Classifier-4096')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T01:21:26.774949Z","iopub.execute_input":"2024-11-14T01:21:26.775725Z","iopub.status.idle":"2024-11-14T01:22:05.713488Z","shell.execute_reply.started":"2024-11-14T01:21:26.775686Z","shell.execute_reply":"2024-11-14T01:22:05.712556Z"}},"outputs":[{"execution_count":162,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/Mail_LongFormer-Classifier-4096.zip'"},"metadata":{}}],"execution_count":162},{"cell_type":"code","source":"from transformers import LongformerForSequenceClassification, LongformerTokenizer\n\n# Load model and tokenizer from the local path\nmodel_dir = \"/kaggle/working/Mail_LongFormer-Classifier-4096.zip\"\nmodel_LF = LongformerForSequenceClassification.from_pretrained(model_dir)\ntokenizer_LF = LongformerTokenizer.from_pretrained(model_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the Fine Tuned Model\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n\n# Load the fine-tuned model\nloaded_model = DistilBertForSequenceClassification.from_pretrained(\"fine_tuned_distilbert\")\n# Load the tokenizer\nloaded_tokenizer = DistilBertTokenizer.from_pretrained(\"fine_tuned_distilbert\")\n\n# Move the loaded model to the appropriate device\nloaded_model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}