{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nilay\\OneDrive - Cal State Fullerton (1)\\Desktop\\NILAY-TO-JOB-DATA\\SPRING 2024\\Projects\\Gmail-JobEmail-Classifier\\myvenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from backend import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>messages</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webinar Confirmation: GenAI: The Shift to Data...</td>\n",
       "      <td>Hi Nilay,\\n\\nThank you for registering for Gen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation: Connor &lt;&gt; Nilay - building RAG job...</td>\n",
       "      <td>When Wednesday Aug 21, 2024 ⋅ 11am – 11:45am (...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you for applying to Johns Hopkins APL!</td>\n",
       "      <td>Dear abc,\\n\\nThank you for applying to the Joh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Garner Health Application Follow Up</td>\n",
       "      <td>Hi Nilay,\\n\\nThank you for applying to our Dat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you for applying to Perplexity AI</td>\n",
       "      <td>Hi Nilay,\\n\\nThank you for applying to the AI ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0  Webinar Confirmation: GenAI: The Shift to Data...   \n",
       "1  Invitation: Connor <> Nilay - building RAG job...   \n",
       "2       Thank you for applying to Johns Hopkins APL!   \n",
       "3                Garner Health Application Follow Up   \n",
       "4            Thank you for applying to Perplexity AI   \n",
       "\n",
       "                                            messages  label  \n",
       "0  Hi Nilay,\\n\\nThank you for registering for Gen...      2  \n",
       "1  When Wednesday Aug 21, 2024 ⋅ 11am – 11:45am (...      2  \n",
       "2  Dear abc,\\n\\nThank you for applying to the Joh...      0  \n",
       "3  Hi Nilay,\\n\\nThank you for applying to our Dat...      3  \n",
       "4  Hi Nilay,\\n\\nThank you for applying to the AI ...      3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV email dataset\n",
    "data_frame = pd.read_csv(os.path.join(config.DATADIR, 'dataset', 'raw', 'sample_1000_email_dataset.csv'))\n",
    "data_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    243\n",
       "1    233\n",
       "2    257\n",
       "3    267\n",
       "Name: messages, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of each label\n",
    "data_frame['messages'].groupby(data_frame['label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nilay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nilay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nilay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nilay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the essential NLTK files for stopwords, punctuations, and lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for removing stopwords, lemmaztize and tokenize the data\n",
    "def clean_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase\n",
    "    # Remove stopwords and punctuation, then lemmatize\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(word) for word in tokens\n",
    "        if word not in stop_words and word not in string.punctuation\n",
    "    ]\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>messages</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webinar Confirmation: GenAI: The Shift to Data...</td>\n",
       "      <td>Hi Nilay,\\n\\nThank you for registering for Gen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation: Connor &lt;&gt; Nilay - building RAG job...</td>\n",
       "      <td>When Wednesday Aug 21, 2024 ⋅ 11am – 11:45am (...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you for applying to Johns Hopkins APL!</td>\n",
       "      <td>Dear abc,\\n\\nThank you for applying to the Joh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0  Webinar Confirmation: GenAI: The Shift to Data...   \n",
       "1  Invitation: Connor <> Nilay - building RAG job...   \n",
       "2       Thank you for applying to Johns Hopkins APL!   \n",
       "\n",
       "                                            messages  label  \n",
       "0  Hi Nilay,\\n\\nThank you for registering for Gen...      2  \n",
       "1  When Wednesday Aug 21, 2024 ⋅ 11am – 11:45am (...      2  \n",
       "2  Dear abc,\\n\\nThank you for applying to the Joh...      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a copy of the original data frame for a separate tokanized data frame\n",
    "data_frame_tokanized = data_frame.copy()\n",
    "data_frame_tokanized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_subject</th>\n",
       "      <th>tokenized_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>webinar confirmation genai shift data intellig...</td>\n",
       "      <td>hi nilay thank registering genai shift data in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>invitation connor nilay building rag job chatb...</td>\n",
       "      <td>wednesday aug 21 2024 ⋅ 11am – 11:45am pacific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>thank applying john hopkins apl</td>\n",
       "      <td>dear abc thank applying john hopkins applied p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>garner health application follow</td>\n",
       "      <td>hi nilay thank applying data analyst role ’ re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>thank applying perplexity ai</td>\n",
       "      <td>hi nilay thank applying ai software engineer r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                  tokenized_subject  \\\n",
       "0      2  webinar confirmation genai shift data intellig...   \n",
       "1      2  invitation connor nilay building rag job chatb...   \n",
       "2      0                    thank applying john hopkins apl   \n",
       "3      3                   garner health application follow   \n",
       "4      3                       thank applying perplexity ai   \n",
       "\n",
       "                                  tokenized_messages  \n",
       "0  hi nilay thank registering genai shift data in...  \n",
       "1  wednesday aug 21 2024 ⋅ 11am – 11:45am pacific...  \n",
       "2  dear abc thank applying john hopkins applied p...  \n",
       "3  hi nilay thank applying data analyst role ’ re...  \n",
       "4  hi nilay thank applying ai software engineer r...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the 'subject' and 'messages' columns and store in 'tokenized_subject' and 'tokenized_messages'\n",
    "data_frame_tokanized['tokenized_subject'] = data_frame_tokanized['subject'].apply(clean_text)\n",
    "data_frame_tokanized['tokenized_messages'] = data_frame_tokanized['messages'].apply(clean_text)\n",
    "data_frame_tokanized = data_frame_tokanized.drop(['subject', 'messages'], axis=1)\n",
    "data_frame_tokanized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_subject</th>\n",
       "      <th>tokenized_messages</th>\n",
       "      <th>email_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>webinar confirmation genai shift data intellig...</td>\n",
       "      <td>hi nilay thank registering genai shift data in...</td>\n",
       "      <td>webinar confirmation genai shift data intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>invitation connor nilay building rag job chatb...</td>\n",
       "      <td>wednesday aug 21 2024 ⋅ 11am – 11:45am pacific...</td>\n",
       "      <td>invitation connor nilay building rag job chatb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>thank applying john hopkins apl</td>\n",
       "      <td>dear abc thank applying john hopkins applied p...</td>\n",
       "      <td>thank applying john hopkins apldear abc thank ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                  tokenized_subject  \\\n",
       "0      2  webinar confirmation genai shift data intellig...   \n",
       "1      2  invitation connor nilay building rag job chatb...   \n",
       "2      0                    thank applying john hopkins apl   \n",
       "\n",
       "                                  tokenized_messages  \\\n",
       "0  hi nilay thank registering genai shift data in...   \n",
       "1  wednesday aug 21 2024 ⋅ 11am – 11:45am pacific...   \n",
       "2  dear abc thank applying john hopkins applied p...   \n",
       "\n",
       "                                       email_content  \n",
       "0  webinar confirmation genai shift data intellig...  \n",
       "1  invitation connor nilay building rag job chatb...  \n",
       "2  thank applying john hopkins apldear abc thank ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tokenized_subject and tokenized_messages\n",
    "data_frame_tokanized['email_content'] = data_frame_tokanized['tokenized_subject'] + data_frame_tokanized['tokenized_messages']\n",
    "data_frame_tokanized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_subject</th>\n",
       "      <th>tokenized_messages</th>\n",
       "      <th>email_content</th>\n",
       "      <th>email_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>webinar confirmation genai shift data intellig...</td>\n",
       "      <td>hi nilay thank registering genai shift data in...</td>\n",
       "      <td>webinar confirmation genai shift data intellig...</td>\n",
       "      <td>[-0.068945006, -0.0932267, -0.00037362284, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>invitation connor nilay building rag job chatb...</td>\n",
       "      <td>wednesday aug 21 2024 ⋅ 11am – 11:45am pacific...</td>\n",
       "      <td>invitation connor nilay building rag job chatb...</td>\n",
       "      <td>[-0.10992242, 0.008053303, 0.09707958, -0.0057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>thank applying john hopkins apl</td>\n",
       "      <td>dear abc thank applying john hopkins applied p...</td>\n",
       "      <td>thank applying john hopkins apldear abc thank ...</td>\n",
       "      <td>[-0.09812444, 0.028659057, 0.0556533, -0.02935...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                  tokenized_subject  \\\n",
       "0      2  webinar confirmation genai shift data intellig...   \n",
       "1      2  invitation connor nilay building rag job chatb...   \n",
       "2      0                    thank applying john hopkins apl   \n",
       "\n",
       "                                  tokenized_messages  \\\n",
       "0  hi nilay thank registering genai shift data in...   \n",
       "1  wednesday aug 21 2024 ⋅ 11am – 11:45am pacific...   \n",
       "2  dear abc thank applying john hopkins applied p...   \n",
       "\n",
       "                                       email_content  \\\n",
       "0  webinar confirmation genai shift data intellig...   \n",
       "1  invitation connor nilay building rag job chatb...   \n",
       "2  thank applying john hopkins apldear abc thank ...   \n",
       "\n",
       "                                    email_embeddings  \n",
       "0  [-0.068945006, -0.0932267, -0.00037362284, -0....  \n",
       "1  [-0.10992242, 0.008053303, 0.09707958, -0.0057...  \n",
       "2  [-0.09812444, 0.028659057, 0.0556533, -0.02935...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using embed model, convert tokenized frame into embeddings\n",
    "model_embedding = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "data_frame_tokanized['email_embeddings'] = data_frame_tokanized['email_content'].apply(model_embedding.encode)\n",
    "data_frame_tokanized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>email_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.068945006, -0.0932267, -0.00037362284, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.10992242, 0.008053303, 0.09707958, -0.0057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.09812444, 0.028659057, 0.0556533, -0.02935...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                   email_embeddings\n",
       "0      2  [-0.068945006, -0.0932267, -0.00037362284, -0....\n",
       "1      2  [-0.10992242, 0.008053303, 0.09707958, -0.0057...\n",
       "2      0  [-0.09812444, 0.028659057, 0.0556533, -0.02935..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_embeddings = data_frame_tokanized.copy()\n",
    "data_frame_embeddings = data_frame_embeddings.drop(['tokenized_subject', 'tokenized_messages', 'email_content'], axis=1)\n",
    "data_frame_embeddings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for model fine-tuning\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>email_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[tensor(-0.0689), tensor(-0.0932), tensor(-0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                   email_embeddings\n",
       "0      2  [tensor(-0.0689), tensor(-0.0932), tensor(-0.0..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the embeddings to tensor format\n",
    "data_frame_embeddings['email_embeddings'] = data_frame_embeddings['email_embeddings'].apply(lambda x: torch.tensor(x))\n",
    "data_frame_embeddings.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(data_frame_embeddings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class to load the data into the format required by transformers\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, df, device=None):\n",
    "        self.input_ids = df['email_embeddings'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
    "        labels_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        if self.device:\n",
    "            input_ids_tensor = input_ids_tensor.to(self.device)\n",
    "            labels_tensor = labels_tensor.to(self.device)\n",
    "        return {'input_ids': input_ids_tensor, 'labels': labels_tensor}\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = EmailDataset(train_df, device='cpu')  # Pass device as needed\n",
    "val_dataset = EmailDataset(val_df, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilBertForSequenceClassification model \n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer_DBSC = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_DBSC = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilay\\OneDrive - Cal State Fullerton (1)\\Desktop\\NILAY-TO-JOB-DATA\\SPRING 2024\\Projects\\Gmail-JobEmail-Classifier\\myvenv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining the training arguments for fine-tuning of the DBSC model.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(config.DATADIR, 'ml_models'),          # Output directory for saving model\n",
    "    num_train_epochs=5,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir=os.path.join(config.DATADIR, 'ml_models', 'logs'),            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation strategy\n",
    "    save_strategy=\"epoch\",           # Save strategy\n",
    "    load_best_model_at_end=True,     # Load the best model when finished training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\630175910.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_DBSC,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Validation dataset\n",
    "    tokenizer=tokenizer_DBSC                  # Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [23:29<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "                                                  \n",
      "  2%|▏         | 10/500 [03:32<2:30:48, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3914, 'grad_norm': 3.272550582885742, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  4%|▍         | 20/500 [06:11<2:07:54, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3895, 'grad_norm': 2.399317502975464, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  6%|▌         | 30/500 [08:49<2:03:25, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3839, 'grad_norm': 2.3556902408599854, 'learning_rate': 3e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  8%|▊         | 40/500 [11:24<1:59:19, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3845, 'grad_norm': 2.388061046600342, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 50/500 [14:12<2:03:45, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3954, 'grad_norm': 2.360504627227783, 'learning_rate': 5e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▏        | 60/500 [17:01<2:08:49, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3839, 'grad_norm': 2.3211779594421387, 'learning_rate': 6e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 14%|█▍        | 70/500 [19:51<1:53:55, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3779, 'grad_norm': 1.8253506422042847, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 16%|█▌        | 80/500 [22:20<1:44:00, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3891, 'grad_norm': 3.7629635334014893, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 18%|█▊        | 90/500 [25:05<1:59:11, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.395, 'grad_norm': 4.3131256103515625, 'learning_rate': 9e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 20%|██        | 100/500 [27:41<1:39:41, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3811, 'grad_norm': 2.844799757003784, 'learning_rate': 1e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                    \n",
      "\n",
      " 20%|██        | 100/500 [29:33<1:39:41, 14.95s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3989975452423096, 'eval_runtime': 112.3452, 'eval_samples_per_second': 1.78, 'eval_steps_per_second': 0.116, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "                                                   \n",
      " 22%|██▏       | 110/500 [32:09<1:45:22, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3914, 'grad_norm': 1.7851414680480957, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 24%|██▍       | 120/500 [34:41<1:32:46, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3835, 'grad_norm': 1.5576601028442383, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 26%|██▌       | 130/500 [37:09<1:30:14, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.399, 'grad_norm': 2.065403461456299, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 28%|██▊       | 140/500 [39:41<1:29:39, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3846, 'grad_norm': 1.287466049194336, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 30%|███       | 150/500 [42:11<1:26:40, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.392, 'grad_norm': 2.494753360748291, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 32%|███▏      | 160/500 [44:46<1:25:14, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4045, 'grad_norm': 1.2497637271881104, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 34%|███▍      | 170/500 [47:16<1:21:20, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4036, 'grad_norm': 2.271383762359619, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 36%|███▌      | 180/500 [49:44<1:18:45, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3987, 'grad_norm': 1.9331021308898926, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 38%|███▊      | 190/500 [52:11<1:16:19, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.376, 'grad_norm': 2.8755645751953125, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 200/500 [54:43<1:14:44, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4164, 'grad_norm': 2.2024426460266113, 'learning_rate': 2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                    \n",
      "\n",
      " 40%|████      | 200/500 [56:30<1:14:44, 14.95s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4012902975082397, 'eval_runtime': 106.637, 'eval_samples_per_second': 1.876, 'eval_steps_per_second': 0.122, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "                                                   \n",
      " 42%|████▏     | 210/500 [59:15<1:22:41, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3831, 'grad_norm': 1.4199646711349487, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 44%|████▍     | 220/500 [1:01:53<1:13:26, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3901, 'grad_norm': 3.175156354904175, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 46%|████▌     | 230/500 [1:04:36<1:12:07, 16.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3952, 'grad_norm': 2.1122868061065674, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 48%|████▊     | 240/500 [1:08:36<1:31:11, 21.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.351, 'grad_norm': 2.7856268882751465, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 250/500 [1:11:18<1:06:49, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4295, 'grad_norm': 2.3211731910705566, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 52%|█████▏    | 260/500 [1:14:05<1:08:25, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4028, 'grad_norm': 1.4549769163131714, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 54%|█████▍    | 270/500 [1:16:51<1:04:15, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3991, 'grad_norm': 2.578497886657715, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 56%|█████▌    | 280/500 [1:19:35<59:41, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4017, 'grad_norm': 1.0915743112564087, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 58%|█████▊    | 290/500 [1:22:14<55:47, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3948, 'grad_norm': 1.2047383785247803, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 300/500 [1:24:53<52:49, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3892, 'grad_norm': 1.337282419204712, 'learning_rate': 3e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                      \n",
      "\n",
      " 60%|██████    | 300/500 [1:26:50<52:49, 15.85s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.391973614692688, 'eval_runtime': 117.5672, 'eval_samples_per_second': 1.701, 'eval_steps_per_second': 0.111, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "                                                     \n",
      " 62%|██████▏   | 310/500 [1:29:39<54:57, 17.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3929, 'grad_norm': 1.3620362281799316, 'learning_rate': 3.1e-05, 'epoch': 3.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 64%|██████▍   | 320/500 [1:32:17<47:21, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3957, 'grad_norm': 1.4959760904312134, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 66%|██████▌   | 330/500 [1:34:55<44:38, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3836, 'grad_norm': 1.99321448802948, 'learning_rate': 3.3e-05, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 68%|██████▊   | 340/500 [1:37:39<45:59, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3822, 'grad_norm': 1.6870115995407104, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 350/500 [1:40:29<40:46, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4021, 'grad_norm': 2.3486757278442383, 'learning_rate': 3.5e-05, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 72%|███████▏  | 360/500 [1:43:08<37:20, 16.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.395, 'grad_norm': 1.4318883419036865, 'learning_rate': 3.6e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 74%|███████▍  | 370/500 [1:45:47<34:30, 15.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3827, 'grad_norm': 1.5419650077819824, 'learning_rate': 3.7e-05, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 76%|███████▌  | 380/500 [1:48:37<33:58, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.388, 'grad_norm': 1.1659373044967651, 'learning_rate': 3.8e-05, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 78%|███████▊  | 390/500 [1:51:22<29:44, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4008, 'grad_norm': 1.483149766921997, 'learning_rate': 3.9000000000000006e-05, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 400/500 [1:54:00<26:21, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3759, 'grad_norm': 2.577026128768921, 'learning_rate': 4e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                      \n",
      "\n",
      " 80%|████████  | 400/500 [1:55:56<26:21, 15.82s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4146380424499512, 'eval_runtime': 115.3516, 'eval_samples_per_second': 1.734, 'eval_steps_per_second': 0.113, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "                                                     \n",
      " 82%|████████▏ | 410/500 [1:58:42<25:41, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3349, 'grad_norm': 2.6703238487243652, 'learning_rate': 4.1e-05, 'epoch': 4.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 84%|████████▍ | 420/500 [2:01:16<20:32, 15.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4341, 'grad_norm': 1.5187342166900635, 'learning_rate': 4.2e-05, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 86%|████████▌ | 430/500 [2:03:47<17:34, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4072, 'grad_norm': 1.7435709238052368, 'learning_rate': 4.3e-05, 'epoch': 4.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 88%|████████▊ | 440/500 [2:06:19<15:09, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3916, 'grad_norm': 1.1443623304367065, 'learning_rate': 4.4000000000000006e-05, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 450/500 [2:08:59<13:02, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3928, 'grad_norm': 1.5602588653564453, 'learning_rate': 4.5e-05, 'epoch': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 92%|█████████▏| 460/500 [2:11:31<10:09, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3953, 'grad_norm': 2.9333488941192627, 'learning_rate': 4.600000000000001e-05, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 94%|█████████▍| 470/500 [2:14:03<07:35, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3789, 'grad_norm': 1.4103642702102661, 'learning_rate': 4.7e-05, 'epoch': 4.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 96%|█████████▌| 480/500 [2:16:35<05:02, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4087, 'grad_norm': 1.889042615890503, 'learning_rate': 4.8e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 98%|█████████▊| 490/500 [2:19:13<02:34, 15.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4148, 'grad_norm': 2.696460723876953, 'learning_rate': 4.9e-05, 'epoch': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 500/500 [2:21:44<00:00, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3974, 'grad_norm': 2.035902261734009, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A                                      \n",
      "\n",
      "100%|██████████| 500/500 [2:23:45<00:00, 15.19s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3923410177230835, 'eval_runtime': 114.085, 'eval_samples_per_second': 1.753, 'eval_steps_per_second': 0.114, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 500/500 [2:23:52<00:00, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8633.0258, 'train_samples_per_second': 0.463, 'train_steps_per_second': 0.058, 'train_loss': 1.3922538299560547, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [2:23:55<00:00, 17.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.3922538299560547, metrics={'train_runtime': 8633.0258, 'train_samples_per_second': 0.463, 'train_steps_per_second': 0.058, 'total_flos': 397416370176000.0, 'train_loss': 1.3922538299560547, 'epoch': 5.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nilay\\AppData\\Local\\Temp\\ipykernel_5560\\970735130.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
      "100%|██████████| 13/13 [01:51<00:00,  8.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.391973614692688,\n",
       " 'eval_runtime': 119.5655,\n",
       " 'eval_samples_per_second': 1.673,\n",
       " 'eval_steps_per_second': 0.109,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 3\n"
     ]
    }
   ],
   "source": [
    "# The email message to classify\n",
    "email_message = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and encode the message\n",
    "inputs = tokenizer_DBSC(email_message, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model_DBSC.eval()\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model_DBSC(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
